{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1 align=\"center\"> Audio Processing with Deep Learning from Zero</h1>\n",
    "    <h3 align=\"center\">Sound Classification, Step-by-Step</h3>\n",
    "    <h4 align=\"center\"><a href=\"http://www.iran-machinelearning.ir\">Soheil Tehranipour</a></h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "**Sound Classification** is one of the most widely used applications in Audio Deep Learning. It involves learning to classify sounds and to predict the category of that sound. This type of problem can be applied to many practical scenarios e.g. classifying music clips to identify the genre of the music, or classifying short utterances by a set of speakers to identify the speaker based on the voice.\n",
    "\n",
    "Just like classifying hand-written digits using the MNIST dataset is considered a ‘Hello World”-type problem for Computer Vision, we can think of this application as the introductory problem for audio deep learning.\n",
    "\n",
    "**We will start with sound files, convert them into spectrograms, input them into a CNN plus Linear Classifier model, and produce predictions about the class to which the sound belongs.**\n",
    "\n",
    "This dataset contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. For a detailed description of the dataset and how it was compiled please refer to our paper.\n",
    "All excerpts are taken from field recordings uploaded to www.freesound.org. The files are pre-sorted into ten folds (folders named fold1-fold10) to help in the reproduction of and comparison with the automatic classification results reported in the article above.\n",
    "\n",
    "In addition to the sound excerpts, a CSV file containing metadata about each excerpt is also provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we will do:\n",
    "\n",
    "we will use the Urban Sound 8K dataset that consists of a corpus of ordinary sounds recorded from day-to-day city life. The sounds are taken from 10 classes such as drilling, dogs barking, and sirens. Each sound sample is labeled with the class to which it belongs.\n",
    "\n",
    "After downloading the dataset, we see that it consists of two parts:\n",
    "\n",
    "* Audio files in the ‘audio’ folder: It has 10 sub-folders named ‘fold1’ through ‘fold10’. Each sub-folder contains a number of ‘.wav’ audio samples eg. ‘fold1/103074–7–1–0.wav’\n",
    "* Metadata in the ‘metadata’ folder: It has a file ‘UrbanSound8K.csv’ that contains information about each audio sample in the dataset such as its filename, its class label, the ‘fold’ sub-folder location, and so on. The class label is a numeric Class ID from 0–9 for each of the 10 classes. eg. the number 0 means air conditioner, 1 is a car horn, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are 3 basic methods to extract features from audio file :\n",
    "\n",
    "* Using the mffcs data of the audio files\n",
    "* Using a spectogram image of the audio and then converting the same to data points (As is done forimages). This is easily done using mel_spectogram function of Librosa\n",
    "* Combining both features to build a better model. (Requires a lot of time to read and extract data).\n",
    "    \n",
    "2. I have chosen to use the second method.\n",
    "\n",
    "3. The labels have been converted to categorical data for classification.\n",
    "\n",
    "4. CNN has been used as the primary layer to classify data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Specific Libraries\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Activation , Dropout\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Data Type and Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many suitable datasets available for sounds of different types. These datasets contain a large number of audio samples, along with a class label for each sample that identifies what type of sound it is, based on the problem you are trying to address.\n",
    "\n",
    "These class labels can often be obtained from some part of the filename of the audio sample or from the sub-folder name in which the file is located. Alternately the class labels are specified in a separate metadata file, usually in TXT, JSON, or CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/urbansound8k/UrbanSound8K.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Column Names\n",
    "\n",
    "* slice_file_name: \n",
    "The name of the audio file. The name takes the following format: [fsID]-[classID]-[occurrenceID]-[sliceID].wav, where:\n",
    "[fsID] = the Freesound ID of the recording from which this excerpt (slice) is taken\n",
    "[classID] = a numeric identifier of the sound class (see description of classID below for further details)\n",
    "[occurrenceID] = a numeric identifier to distinguish different occurrences of the sound within the original recording\n",
    "[sliceID] = a numeric identifier to distinguish different slices taken from the same occurrence\n",
    "\n",
    "* fsID:\n",
    "The Freesound ID of the recording from which this excerpt (slice) is taken\n",
    "\n",
    "* start\n",
    "The start time of the slice in the original Freesound recording\n",
    "\n",
    "* end:\n",
    "The end time of slice in the original Freesound recording\n",
    "\n",
    "* salience:\n",
    "A (subjective) salience rating of the sound. 1 = foreground, 2 = background.\n",
    "\n",
    "* fold:\n",
    "The fold number (1-10) to which this file has been allocated.\n",
    "\n",
    "* classID:\n",
    "A numeric identifier of the sound class:\n",
    "0 = air_conditioner\n",
    "1 = car_horn\n",
    "2 = children_playing\n",
    "3 = dog_bark\n",
    "4 = drilling\n",
    "5 = engine_idling\n",
    "6 = gun_shot\n",
    "7 = jackhammer\n",
    "8 = siren\n",
    "9 = street_music\n",
    "\n",
    "* class:\n",
    "The class name: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, \n",
    "siren, street_music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Librosa to analyse random sound sample - SPECTROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1, sampling_rate1 = librosa.load('../input/urbansound8k/fold5/100032-3-0-0.wav')\n",
    "dat2, sampling_rate2 = librosa.load('../input/urbansound8k/fold5/100263-2-0-117.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(dat1)), ref=np.max)\n",
    "plt.subplot(4, 2, 1)\n",
    "librosa.display.specshow(D, y_axis='linear')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Linear-frequency power spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(dat2)), ref=np.max)\n",
    "plt.subplot(4, 2, 1)\n",
    "librosa.display.specshow(D, y_axis='linear')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Linear-frequency power spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(df[\"slice_file_name\"])\n",
    "fold = np.array(df[\"fold\"])\n",
    "cla = np.array(df[\"class\"])\n",
    "\n",
    "for i in range(192, 197, 2):\n",
    "    path = '../input/urbansound8k/fold' + str(fold[i]) + '/' + arr[i]\n",
    "    data, sampling_rate = librosa.load(path)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(data)), ref=np.max)\n",
    "    plt.subplot(4, 2, 1)\n",
    "    librosa.display.specshow(D, y_axis='linear')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(cla[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction and Database Building\n",
    "\n",
    "## Mel Spectrograms\n",
    "\n",
    "* This raw audio is now converted to Mel Spectrograms. A Spectrogram captures the nature of the audio as an image by decomposing it into the set of frequencies that are included in it.\n",
    "\n",
    "## MFCC\n",
    "\n",
    "* For human speech, in particular, it sometimes helps to take one additional step and convert the Mel Spectrogram into MFCC (Mel Frequency Cepstral Coefficients). MFCCs produce a compressed representation of the Mel Spectrogram by extracting only the most essential frequency coefficients, which correspond to the frequency ranges at which humans speak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n",
    "\n",
    "1. I have used Librosa to extract features.\n",
    "2. To do so, I will go through each fold and extract the data for each file. Then I have used the mel_spectogram function of librosa to extract the spectogram data as a numpy array.\n",
    "3. After reshaping and cleaning the data, 80-20 split has been performed.\n",
    "4. Classes (Y) have been converted to Categorically Encoded Data usng Keras.utils\n",
    "\n",
    "Note : Running the parser function may take few minutes depending on your system since it has to extract spectogram data for 8732 audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extract(file):\n",
    "    sample,sample_rate = librosa.load(file_name,res_type='kaiser_fast')\n",
    "    feature = librosa.feature.mfcc(y=sample,sr=sample_rate,n_mfcc=50)\n",
    "    scaled_feature = np.mean(feature.T,axis=0)\n",
    "    return scaled_feature\n",
    "\n",
    "extracted = []\n",
    "path = '../input/urbansound8k/'\n",
    "\n",
    "for index_num,row in tqdm(df.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(path),'fold'+str(row[\"fold\"])+'/',str(row['slice_file_name'])) \n",
    "    final_class_labels = row['class']   \n",
    "    data= features_extract(file_name)    \n",
    "    extracted.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_df = pd.DataFrame(extracted,columns=['feature','class'])\n",
    "ext_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(ext_df['feature'].tolist())\n",
    "y = np.array(ext_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y = to_categorical(le.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "print(\"Number of training samples = \", x_train.shape[0])\n",
    "print(\"Number of testing samples = \",x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = y.shape[1]\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_shape=(50,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "          x_train, \n",
    "          y_train, \n",
    "          batch_size=32, \n",
    "          epochs=100,\n",
    "          validation_data=(x_test, y_test),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    audio_data, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "    fea = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=50)\n",
    "    scaled = np.mean(fea.T,axis=0)\n",
    "    return np.array([scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(file_name):\n",
    "    pred_fea = extract_feature(file_name) \n",
    "    pred_vector = np.argmax(model.predict(pred_fea), axis=-1)\n",
    "    pred_class = le.inverse_transform(pred_vector)\n",
    "    print(\"The predicted class is:\", pred_class[0], '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prediction('../input/urbansound8k/fold2/100652-3-0-2.wav')\n",
    "ipd.Audio('../input/urbansound8k/fold2/100652-3-0-2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prediction('../input/urbansound8k/fold5/100263-2-0-137.wav')\n",
    "ipd.Audio('../input/urbansound8k/fold5/100263-2-0-137.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prediction('../input/urbansound8k/fold7/102853-8-0-2.wav')\n",
    "ipd.Audio('../input/urbansound8k/fold7/102853-8-0-2.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
