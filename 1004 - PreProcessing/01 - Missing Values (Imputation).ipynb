{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1 align=\"center\">Missing Values (Imputation) in a Nutshell</h1>\n",
    "    <h3 align=\"center\">PreProcessing before Training model</h3>\n",
    "    <h4 align=\"center\"><a href=\"http://www.iran-machinelearning.ir\">Soheil Tehranipour</a></h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values is one of the most common problems that you will come across when performing data analysis. Furthermore, machine learning models require that a dataset does not contain any missing values before they can be fitted to the data. Therefore, it is crucial that we learn how to properly handle them.\n",
    "\n",
    "There are mainly two ways that we can deal with missing values in a dataset:\n",
    "\n",
    "1. If there are only a few rows with missing values or if a column has an overwhelming number of missing values, we can simply drop them from the dataset without running into the risk of losing too much information. \n",
    "2. An alternative approach to handling missing values is via imputation. Imputation is the process whereby the missing values in a dataset are replaced with some substituted values. \n",
    "\n",
    "For the purpose of this tutorial, we will examine the process of imputation and more specifically, we will learn the difference between a univariate approach to imputing versus a multivariate approach to imputing. \n",
    "\n",
    "Univariate imputation implies that we are only considering the values of a single column when performing imputation. Multivariate imputation, on the other hand, involves taking into account other features in the dataset when performing imputation.\n",
    "\n",
    "The multivariate approach to imputing is generally preferred over the univariate approach as it is more robust and provides our model with a more accurate representation of the missing values in order to make better predictions.\n",
    "\n",
    "In this tutorial, we will explore 3 different imputation techniques with reference to the [Titanic dataset](https://www.kaggle.com/c/titanic/data) on Kaggle:\n",
    "\n",
    "- [Simple imputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "- [Iterative imputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer)\n",
    "- [KNN Imputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# Data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Import and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\soso\\Desktop\\Maktbakhoone\\Scikit-Learn Tips\\00 -  Missing Value/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Drop unwanted columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, I proceeded to drop the PassengerId, Name, Ticket and Cabin columns from the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop PassengerId, Name, Ticket and Cabin columns \n",
    "\n",
    "data = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Drop rows with missing Embarked values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data\n",
    "\n",
    "missing = data.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending = False)\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are only two rows with missing Embarked values, I decided to simply drop those two rows from the dataset, leaving us with 177 rows with missing Age values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing Embarked values \n",
    "\n",
    "print(\"Before: \", data.shape)\n",
    "data = data.loc[data['Embarked'].notnull(), :]\n",
    "print(\"After: \", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values again\n",
    "\n",
    "missing = data.isnull().sum()\n",
    "missing[missing > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Explore Age feature\n",
    "\n",
    "Before we perform imputation on the Age column, let's briefly explore the Age feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap \n",
    "\n",
    "correlation = data.corr()\n",
    "plt.figure(figsize = (7, 5))\n",
    "sns.heatmap(correlation, annot = True, fmt = '.2f', cmap = 'coolwarm')\n",
    "plt.title('Correlation between features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age correlation with other features \n",
    "\n",
    "correlation['Age'].sort_values(ascending = False)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Age feature is slightly positively correlated with the Fare feature. In other words, passengers that are older generally pay a higher fare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot passenger age distribution\n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "sns.distplot(data['Age'], label = 'Skewness: {:.2f}'.format(data['Age'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Passenger Age Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age summary statistics \n",
    "\n",
    "data['Age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Missing values imputation\n",
    "\n",
    "In this section, we will look at 3 different imputation techniques:\n",
    "\n",
    "1. Simple imputer\n",
    "2. Iterative imputer\n",
    "3. KNN Imputer\n",
    "\n",
    "Here, I have created a sample dataframe to demonstrate the differences between the 3 techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataframe\n",
    "\n",
    "df = pd.DataFrame({'SibSp': [1, 1, 0, 1, 0, 0], \n",
    "                   'Fare': [7.25, 71.2833, 7.925, 53.1, 8.05, 8.4583], \n",
    "                   'Age': [22, 38, 26, 35, 35, np.nan]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Simple imputer\n",
    "\n",
    "Simple imputer follows a univariate approach to imputing missing values i.e. it only takes a single feature into consideration. Some of the most common uses of simple imputer are:\n",
    "\n",
    "- Mean\n",
    "- Median\n",
    "- Most frequent (mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average age: \", df['Age'].mean())\n",
    "simple_imp = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "simple_imp.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, simple imputer has filled the missing value in the Age column with the average age which is 31.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Iterative imputer\n",
    "\n",
    "Iterative imputer is an example of a multivariate approach to imputation. It models the missing values in a column by using information from the other columns in a dataset. More specifically, it treats the column with missing values as a target variable while the remaining columns are used are predictor variables to predict the target variable. \n",
    "\n",
    "In our sample data frame, the Age column has one missing value on row 6 and is therefore assigned as the target variable in this scenario. This leaves the SibSp and Fare columns as our predictor variables. \n",
    "\n",
    "Iterative imputer will use the first 5 rows of the data frame to train a predictive model. Once the model is ready, it will then values in the SibSp and Fare columns of row 6 as inputs and predict the Age value for that row.\n",
    "\n",
    "#### It is a data transform that is first configured based on the method used to estimate the missing values. By default, a BayesianRidge model is employed that uses a function of all other input features. Features are filled in ascending order, from those with the fewest missing values to those with the most.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_imp = IterativeImputer()\n",
    "iterative_imp.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the value predicted under iterative imputer is different to that under simple imputer.\n",
    "\n",
    "This is a more accurate approach to predict the missing Age value as it takes other features in our dataframe into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 KNN imputer\n",
    "\n",
    "Last but not least, we have KNN Imputer which is another multivariate imputation technique. KNN Imputer scans our dataframe for k nearest observations to the row with missing value. It will then proceed to fill the missing value with the average of those nearest observations. \n",
    "\n",
    "Here, I have set k to equal to 2 or in other words, I want KNN imputer to look for 2 observations that are nearest to row 6 and fill the missing age with the average age of those 2 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imp = KNNImputer(n_neighbors = 2)\n",
    "knn_imp.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, KNN imputer has taken row 3 and row 5 as the nearest observations for row 6.\n",
    "\n",
    "Therefore, the average age between row 3 and row 5 is (26 + 35) / 2 = 30.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model accuracy under simple imputer and iterative imputer\n",
    "\n",
    "Now that we have a better understanding of how the different imputers work, we can move on to apply these techniques to our Titanic dataset and compare the model accuracy under each approach.\n",
    "\n",
    "We should expect to see our model perform better under multivariate imputation than univariate imputation as multivariate imputation provides a more accurate prediction of the missing values and thus allowing our model to make better predictions. \n",
    "\n",
    "In this section, we will build a column transformer which consists of a OneHotEncoder for encoding the Sex and Embarked columns as well as an imputer to impute the missing values in the Age column.\n",
    "\n",
    "Following that, we will chain the column transformer with a random forest classifier to predict the surival of the passengers on the Titanic. Finally, we will perform 10-fold cross-validation to compare the prediction results under univariate imputation versus under multivariate imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictor and target variables \n",
    "\n",
    "X = data.drop('Survived', axis = 1)\n",
    "Y = data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor and target variables shape\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Univariate imputation (simple imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate OneHotEncoder for Sex and Embarked columns\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# Instantiate simple imputer \n",
    "simple_imp = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "\n",
    "# Instantiate model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Make column transformer with simple imputer\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['Sex', 'Embarked']),\n",
    "    (simple_imp, ['Age']),\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "# Pipeline\n",
    "pipe = make_pipeline(column_transform, rf)\n",
    "\n",
    "# 10-fold cross-validation\n",
    "cross_val_score(pipe, X, Y, cv = 10, scoring = 'accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Multivariate imputation (iterative imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate OneHotEncoder for Sex and Embarked columns\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "# Instantiate iterative imputer\n",
    "iterative_imp = IterativeImputer()\n",
    "\n",
    "# Instantiate model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Make column transformer with iterative imputer\n",
    "column_transform = make_column_transformer(\n",
    "    (ohe, ['Sex', 'Embarked']),\n",
    "    (iterative_imp, ['Age']),\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "# Pipeline\n",
    "pipe = make_pipeline(column_transform, rf)\n",
    "\n",
    "# 10-fold cross-validation \n",
    "cross_val_score(pipe, X, Y, cv = 10, scoring = 'accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not a drastic difference, we can conclude that our model performed better under multivariate imputation as it achieved a higher mean cross-validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we have learned the difference between univariate imputation and multivariate imputation. In addition, We also looked at 3 different imputation techniques within Scikit-learn which include simple imputer, iterative imputer and KNN imputer. \n",
    "\n",
    "After comparing the prediction accuracy of our model using simple imputer versus iterative imputer, we can conclude that multivariate imputation results in better model predictions due to its lower mean cross-validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. References\n",
    "\n",
    "- [Scikit-learn imputation of missing values](https://scikit-learn.org/stable/modules/impute.html)\n",
    "- [Impute missing values using KNNImputer and IterativeImputer](https://www.youtube.com/watch?v=m_qKhnaYZlc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://webna.ir/wp-content/uploads/2018/08/%D9%85%DA%A9%D8%AA%D8%A8-%D8%AE%D9%88%D9%86%D9%87.png\" width=50% />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
