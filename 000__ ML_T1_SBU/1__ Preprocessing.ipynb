{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "\n",
    "- [Getting Setup](#Getting-Set-Up)\n",
    "    - [Load and View Data](#Load-and-View-Data)\n",
    "    \n",
    "    \n",
    "- [Handling Missing Values ](#Handling-Missing-Values)\n",
    "    - [Types of Missing Data](#Types-of-Missing-Data)\n",
    "    - [Identifying Missing Data](#Identifying-missing-data)\n",
    "    - [Dropping the null-values](#Dropping-the-null-values)\n",
    "    - [Mean/Median Imputation](#Mean/Median-Imputation)\n",
    "        - [`.fillna()` function](#let’s-discuss-the-.fillna()-function) \n",
    "        - [scikit-learn SimpleImputer](#scikit-learn-SimpleImputer)\n",
    "    - [Random Sample Imputation](#Random-Sample-Imputation)\n",
    "    - [Multivariate Imputation](#Multivariate-Imputation)\n",
    "        - [MICE Algorithm](#MICE-Algorithm)\n",
    "        \n",
    "        \n",
    "- [Categorical Feature Encoding](#Categorical-Feature-Encoding)\n",
    "    - [Label Encoding](#Label-Encoding)\n",
    "    - [Ordinal Encoding](#Ordinal-Encoding)\n",
    "    - [One-Hot Encoding](#One-Hot-Encoding)\n",
    "    - [Count & Frequency Encoding](#Count-&-Frequency-Encoding)\n",
    "    - [Target Encoding](#Target-Encoding)\n",
    "    \n",
    "    \n",
    "- [Feature Scaling](#Feature-Scaling)\n",
    "    - [Standardization (Z-score normalization)](#Standardization-(Z-score-normalization):)\n",
    "    - [Normalization (Min-Max scaling)](#Normalization-(Min-Max-scaling):)\n",
    "    - [RobustScaler](#RobustScaler:)\n",
    "    - [MaxAbsScaler](#MaxAbsScaler:)\n",
    "    \n",
    "    \n",
    "- [Imbalanced Datasets](#Imbalanced-Datasets)\n",
    "    - [Sampling Methods](#Sampling-Methods)\n",
    "        - [Random Oversampling](#Random-Oversampling) \n",
    "        - [Random Underampling](#Random-Underampling)\n",
    "        - [SMOTE](#SMOTE) \n",
    "        - [ADASYN](#ADASYN) \n",
    "    - [Choosing the Proper Algorithm](#Choosing-the-Proper-Algorithm)\n",
    "    - [Modifying the Loss Function](#Modifying-the-Loss-Function)\n",
    "    - [Confusion Matrix](#Confusion-Matrix)\n",
    "\n",
    "\n",
    "- [Outlier Detection](#Outlier-Detection)\n",
    "    - [Ways to Detect Outliers](#Ways-to-Detect-Outliers)\n",
    "    - [Approaches to Treat Outliers](#Approaches-to-Treat-Outliers)\n",
    "        - [Z-Score Method](#Z-score-Method:) \n",
    "        - [IQR-Based Method](#IQR-Based-Method:)\n",
    "        - [Percentile Method](#Percentile-Method:) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Getting Set Up\n",
    "### Load and View Data \n",
    "\n",
    "Here, we'll just do the routine basics; loading relevant libraries and our data.\n",
    "\n",
    "Same as previous sessions, we'll be using data from the [Spaceship Titanic](#https://www.kaggle.com/competitions/spaceship-titanic/) competition on Kaggle, and using that for today. Just as a reminder, this is a binary classification task, where we're looking to predict the `Transported` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries (we'll import more libraries further down in the notebook on an as-needed basis)\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "file_path = \"./spaceship-titanic.csv\" # write the path of your file\n",
    "raw_data = pd.read_csv(file_path, header=0) \n",
    "df = raw_data.copy(deep=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Handling Missing Values \n",
    "\n",
    "In data analysis, missing values pose a significant challenge that can compromise the **accuracy** and **reliability** of results. Identifying and handling missing values is a critical step in any data analysis process, as it ensures that the data is complete and representative of the underlying population.\n",
    "\n",
    "Missing values can arise due to various factors, such as data collection errors, incomplete surveys, or technical glitches. If left unaddressed, they can:\n",
    "\n",
    "* Bias statistical analyses by reducing sample size and introducing skewness\n",
    "* Compromise the validity of conclusions drawn from the data\n",
    "* Limit the potential of machine learning models by hindering their ability to generalize\n",
    "\n",
    "### Types of Missing Data\n",
    "\n",
    "Missing data can be classified into three main types:\n",
    "\n",
    "**1. Missing Completely at Random (MCAR)**\n",
    "\n",
    " The probability of a data point being missing is the **same for all** observations. There is **no relationship** between the missingness of the data and any values, observed or missing.\n",
    " \n",
    "* Example: A survey respondent forgets to answer a question due to a technical issue.\n",
    "\n",
    "* Example: Data is lost due to a technical glitch during data transfer or storage.\n",
    "\n",
    "**2. Missing at Random (MAR)**\n",
    "\n",
    " The probability of a data point being missing is not the same for all observations, but the missingness **depends on some observed variables** in the dataset.\n",
    " \n",
    "* Example: A survey respondent skips a question because they do not meet a certain eligibility criterion.\n",
    "\n",
    "* Example: A feedback form where customers who had a negative experience are more likely to skip satisfaction rating questions.\n",
    "\n",
    "**3. Missing Not at Random (MNAR)**\n",
    "\n",
    " The missingness of the data is **related to the reason it is missing**. That is, the probability of a data point being missing is related to the unobserved data.\n",
    "* Example: A survey respondent refuses to answer a question due to its sensitive nature.\n",
    "\n",
    "* Example: Couples with a bad relationship might not want to answer certain questions as they might feel embarrassed to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Identifying missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "This method provides a concise summary of a DataFrame, including the number of **non-null entries** for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].isnull() # you can also use .isna() instead "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "These methods return a **DataFrame** or **Series** indicating whether each element is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .sum() used after .isnull(), it can give a count of missing values for each column.\n",
    "df.iloc[0].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "These methods are instrumental in the initial stages of data cleaning and preparation, allowing data scientists to make informed decisions about how to deal with missing data in their datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Dropping the null values \n",
    "The `.dropna()` method in pandas is a tool for handling missing data. It allows you to remove missing values from a DataFrame or Series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the rows with atleast one null value \n",
    "df.dropna().isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "We can see that all the observations are dropped from the dataset, which can be especially dangerous for the rest of the analysis.\n",
    "\n",
    "---\n",
    "Here are some of the key features of `.dropna()` that enable us to tailor its functionality effectively:\n",
    "\n",
    "**`axis`**: Determines whether to drop rows or columns that contain missing values.\n",
    "\n",
    "  - `axis=0` or `axis='index'`: Drop rows which contain missing values.(Default) \n",
    "  - `axis=1` or `axis='columns'`: Drop columns which contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops all the columns with missing values.\n",
    "df.dropna(axis=1).isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "all the columns have at least one missing value except `PassengerId` and\n",
    "`Transported`.\n",
    "\n",
    "---\n",
    "**`how`**: Decides if a row or column should be dropped from the DataFrame when we have at least one NA or all NAs.\n",
    "  - `how='any'`: Drop if any NA values are present.(Deafalut) \n",
    "  - `how='all'`: Drop only if all values are NA.\n",
    "  \n",
    "  \n",
    "  \n",
    "**`thresh`**: Requires that many non-NA values to not drop the row or column. Cannot be combined with how.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**`subset`**: Defines in which columns to look for missing values.\n",
    "\n",
    "**`ignore_indexbool`**: default `False`, If `True`, the resulting axis will be labeled 0, 1, …, n - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows where all the values of a subset of columns are missing\n",
    "df.dropna(how='all', subset=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']).isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Nothing has changed!\n",
    "\n",
    "**Note**: You can use `inplace=True` with `.dropna()` to directly remove missing values from your DataFrame. **Always ensure you have a backup of your data before using this option**, as changes made are irreversible and will modify your original dataset.\n",
    "\n",
    "---\n",
    "\n",
    "Here are the pros and cons of using `.dropna()` method:\n",
    "\n",
    "**Pros:**\n",
    "- `.dropna()` is easy to use and can quickly remove missing values from a dataset.\n",
    "\n",
    "- Beneficial when missing values have no importance. \n",
    "\n",
    "**Cons:**\n",
    "- Can result in significant **data loss**, especially if not used carefully, which may affect the analysis.\n",
    "\n",
    "- If the data is not missing completely at random, dropping it can introduce bias into the dataset.\n",
    "\n",
    "- Missing values can sometimes contain useful information, and removing them might lead to overlooking important patterns or trends.\n",
    "\n",
    "Using `.dropna()` can be a double-edged sword; it's a quick fix for missing values but can potentially lead to the loss of valuable insights if the missingness is informative or the dataset is small. Alternative methods, such as imputation, should be considered to preserve data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Mean/Median Imputation\n",
    "Mean/median imputation is a method used to fill in missing values with the **mean** or **median** of the available data. This technique is based on the assumption that the missing values are roughly equivalent to the average of the existing values. \n",
    "\n",
    "This approach is particularly useful when the data is believed to be **missing completely at random (MCAR)**, and the proportion of missing data is relatively small.\n",
    "\n",
    "### Mean Vs Median Imputation \n",
    "Mean and median imputation are practical methods for filling in missing data, each with its own ideal use case. Mean imputation is best suited for data that is **normally distributed** and **free of outliers**, as it preserves the overall mean but can underestimate variance. \n",
    "\n",
    "Median imputation, conversely, is more **robust** and preferable for **skewed distributions** or when outliers are present, as it is less influenced by extreme values and does not distort the dataset’s **central tendency** as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the .fillna() function to make these replacements\n",
    "before = df['Spa'].isnull().sum()\n",
    "mean = df['Spa'].mean()\n",
    "df['Spa'] = df.Spa.fillna(mean)\n",
    "after = df['Spa'].isnull().sum()\n",
    "print(f'Before Mean Imputation : {before}\\nAfter Mean Imputation {after}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### let’s discuss the `.fillna()` function \n",
    "\n",
    "The `.fillna()` function in pandas is a versatile method for handling missing data by filling the NA/NaN values in a DataFrame. Here's an overview of its features:\n",
    "\n",
    "- **`value`**: Specifies the **scalar** value or **dictionary** object to use for filling missing entries.\n",
    "\n",
    "\n",
    "- **`method`**: Determines the method to use for filling holes in reindexed Series (`'ffill'` for forward fill, `'bfill'` for backward fill).\n",
    "\n",
    "\n",
    "- **`axis`**: Chooses the axis along which to fill missing values (`0` or `'index'` for index, `1` or `'columns'` for columns).\n",
    "\n",
    "\n",
    "- **`limit`**: Sets the maximum number of **consecutive** NA values to fill.\n",
    "\n",
    "Each of these features allows you to control how and where the missing values are filled, making `.fillna()` a flexible tool for data preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using fillna()\n",
    "df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n",
    "                   [3, 4, np.nan, 1],\n",
    "                   [np.nan, np.nan, np.nan, np.nan],\n",
    "                   [np.nan, 3, np.nan, 4]],\n",
    "                  columns=list(\"ABCD\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with a scalar value\n",
    "df.fillna(value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values using forward fill method\n",
    "df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values using backward fill method along columns\n",
    "df.fillna(method='bfill', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### scikit-learn SimpleImputer\n",
    "`SimpleImputer` is a transformer in scikit-learn that provides a simple and efficient way to handle missing values in a dataset. It supports a variety of imputation strategies, including replacing missing values with the mean, median, or most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using SimpleImputer for mean imputation \n",
    "from sklearn.impute import SimpleImputer\n",
    "df = raw_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['Age', 'Spa']] = imputer.fit_transform(df[['Age', 'Spa']])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "---\n",
    "#### `SimpleImputer` Features:\n",
    "\n",
    "* **missing_values:** The value to be imputed for missing values. Can be a string ('NaN', '?', etc.), a number (0, -1, etc.), or a placeholder object (e.g., `np.nan`).\n",
    "\n",
    "\n",
    "* **strategy:** The imputation strategy to use. Options include:\n",
    "    * 'mean': Replace missing values with the mean of the non-missing values in the column. Suitable for imputing missing values in **numerical** columns that are **normally distributed**.\n",
    "    \n",
    "    * 'median': Replace missing values with the median of the non-missing values in the column. Suitable for imputing missing values in **numerical** columns that are not normally distributed or **have outliers**.\n",
    "    \n",
    "    * 'most_frequent': Replace missing values with the most frequent value in the column. Suitable for imputing missing values in **categorical** columns or **numerical** columns with a **limited number** of unique values.\n",
    "    \n",
    "    * 'constant': Replace missing values with a constant value specified by the `fill_value` parameter.\n",
    "    \n",
    "    \n",
    "* **fill_value:** The constant value to use for imputation when the `strategy` is set to 'constant'.\n",
    "\n",
    "* **add_indicator:** If set to `True`, it adds a binary indicator matrix to the output, marking the presence of missing values.\n",
    "\n",
    "* **keep_empty_features:**  If set to `True`, it will keep the features that have all missing values during the imputation process.\n",
    "---\n",
    "#### Why Add an Indicator?\n",
    "\n",
    "Adding an indicator column to the output dataset can be useful in several scenarios:\n",
    "\n",
    "* `Identifying imputed values`: The indicator column can be used to identify the rows or columns that have been imputed, which can be useful for **debugging** or further analysis.\n",
    "\n",
    "\n",
    "* `Evaluating imputation quality`: The indicator column can be used to assess the **quality of the imputation** by comparing the imputed values to the true values (if available).\n",
    "\n",
    "\n",
    "* `Modeling missing data`: The indicator column can be used as a **feature** in machine learning models to account for the presence of missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for using SimpleImputer\n",
    "df = pd.DataFrame([[np.nan, 2, np.nan, 0],\n",
    "                   [3, 4, np.nan, 1],\n",
    "                   [np.nan, np.nan, np.nan, np.nan],\n",
    "                   [np.nan, 3, np.nan, 4]],\n",
    "                  columns=list(\"ABCD\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleImputer instance with constant strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-1)\n",
    "dfi = imputer.fit_transform(df)\n",
    "dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleImputer instance with mean strategy\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "dfi = imputer.fit_transform(df)\n",
    "dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleImputer instance with most frequent strategy with an indicator\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent', add_indicator=True)\n",
    "dfi = imputer.fit_transform(df)\n",
    "dfi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "The output of the SimpleImputer is a **NumPy array** that contains the **imputed values** and an **indicator columns**. Each row in the array corresponds to a row in the original DataFrame\n",
    "**Summary and Paraphrase:**\n",
    "\n",
    "\n",
    "* The first 3 columns of the array contain the imputed values. **Missing values** in the original DataFrame have been replaced with the **most frequent value** in the corresponding column.\n",
    "\n",
    "\n",
    "* The 4 columns after are the **indicator columns**. It contains **1**s for values that were **imputed** and **0**s for values that were **not imputed**.\n",
    "\n",
    "\n",
    "* Note that the **3th column** of the original data is **dropped** in the output. (check `keep_empty_features`) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Random Sample Imputation\n",
    "Random Sample Imputation is a technique used to handle missing data by replacing the missing values with a **random sample** from the **pool of available observations** in the data. The idea is to preserve the **original distribution** of the data by randomly selecting values to fill in the gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example of Random Sample Imputation\n",
    "def random_sample_imputation(df, column_name):\n",
    "    df[column_name + '_imputed'] = df[column_name]\n",
    "    \n",
    "    # Get a random sample to fill the na values\n",
    "    random_sample = df[column_name].dropna().sample(df[column_name].isnull().sum(), random_state=0)\n",
    "    \n",
    "    # Pandas needs to have the same index in order to merge datasets\n",
    "    random_sample.index = df[df[column_name].isnull()].index\n",
    "    df.loc[df[column_name].isnull(), column_name + '_imputed'] = random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data.copy(deep=True)\n",
    "random_sample_imputation(df, 'Age')\n",
    "df.isnull().sum() # check the last column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to impute missing values with random sample imputation\n",
    "df['Age'] = df['Age'].fillna(np.random.choice(df['Age'].dropna()))\n",
    "df.isnull().sum() # check the Age column "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "---\n",
    "**Pros:**\n",
    "- **Preserves Distribution**: It maintains the statistical properties of the original data since the imputation is random.\n",
    "\n",
    "\n",
    "- **Flexibility**: Can be used for both categorical and numerical data.\n",
    "\n",
    "**Cons:**\n",
    "- **Randomness**: The randomness can introduce noise into the dataset, especially if the missing data is not completely at random.\n",
    "\n",
    "\n",
    "- **Reproducibility**: Unless a random seed is set, the results may vary each time the imputation is performed, which can affect reproducibility.\n",
    "\n",
    "\n",
    "- **Data Integrity**: It does not account for the relationships between features, which might lead to a loss of information about the data structure.\n",
    "\n",
    "Random Sample Imputation can be a useful tool when the goal is to retain the original distribution of the data without introducing bias. However, careful consideration should be given to the nature of the missing data and the context of the analysis when choosing this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "---\n",
    "## Multivariate Imputation\n",
    "\n",
    "Machine-learning models can be used to impute missing values in a single column by predicting the missing values based on the other columns in the dataset. This approach is known as **predictive imputation**.\n",
    "\n",
    "One common method for predictive imputation is to use a **regression model**, such as linear regression or decision trees. The model is trained on the non-missing values in the column, and then used to predict the missing values.\n",
    "\n",
    "Another method for predictive imputation is to use a **k-nearest neighbors (k-NN)** algorithm. k-NN finds the k most similar rows to the row with the missing value, and then uses the values from those rows to impute the missing value.\n",
    "\n",
    "(checkout [sklearn.impute.KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html))\n",
    "\n",
    "\n",
    "**Multivariate Imputation** is a more advanced technique that imputes missing values in **multiple columns** simultaneously. It takes into account the relationships between the different variables in the dataset, and imputes the missing values in a way that preserves these relationships.\n",
    "\n",
    "(checkout [sklearn.impute.IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html))\n",
    "\n",
    "### MICE Algorithm\n",
    "\n",
    "One popular multivariate imputation method is the **Multiple Imputation by Chained Equations (MICE)** algorithm. MICE is an **iterative method** that models each feature with missing values as a function of other features in a **round-robin fashion**. It works in the following steps:\n",
    "\n",
    "1. **Initial Imputation**: Missing values are filled in with initial guesses, which could be mean, median, or a random sample from the observed values.\n",
    "2. **Iterative Process**: Each feature with missing values is modeled in turn, using the other features as predictors. The missing values are then updated with predictions from this model.\n",
    "3. **Repetition**: Steps 1 and 2 are repeated for a number of iterations, allowing the model to converge to a more accurate estimate of the missing values.\n",
    "\n",
    "\n",
    "MICE is a powerful tool for dealing with missing data, especially when the missingness is believed to be **related to other observed variables** in the dataset. However, it requires a good understanding of the underlying data and careful implementation to achieve the best results.\n",
    "\n",
    "---\n",
    "**Pros:** \n",
    "- **Powerful**: Effectively handles missing data across multiple variables and data types.\n",
    "- **Superior Results**: Outperforms mean and median imputations, providing better results.\n",
    "- **Algorithm Flexibility**: Supports various algorithms like K-Nearest Neighbors, Random Forest, and Neural Networks for prediction.\n",
    "\n",
    "**Cons**\n",
    "- **Assumption of MAR**: Operates under the assumption that data is missing at random.\n",
    "- **Computational Expense**: Can be resource-intensive, particularly with large datasets.\n",
    "- **Increased Effort**: Demands more effort to implement than simpler imputation methods.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Table Of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "# Categorical Feature Encoding\n",
    "\n",
    "Categorical features, also known as **nominal** features, are a type of data that represents **qualitative** information rather than numerical values. They can take on a limited, and usually fixed, number of possible values. Unlike numerical features, which can be ordered and manipulated mathematically, categorical features are discrete and often have no inherent order.\n",
    "\n",
    "Examples include gender (male, female), color (red, green, blue), or the presence of a disease (yes, no). Categorical features are often further classified into **two types**:\n",
    "\n",
    "1. **Nominal Features**: These are categorical values without any intrinsic ordering or ranking. For example, country names, types of weather, or movie genres. There's no way to sort these categories in a meaningful way.\n",
    "2. **Ordinal Features**: These have a clear ordering or ranking. For instance, educational level (high school, bachelor's, master's, doctorate) or shirt sizes (small, medium, large).\n",
    "\n",
    "We use **encoding** for categorical features because most machine learning algorithms work with numerical input and output values. Encoding transforms categorical data into a format that can be provided to ML algorithms to do a better job in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "file_path = \"./spaceship-titanic.csv\" # write the path of your file\n",
    "raw_data = pd.read_csv(file_path, header=0) \n",
    "df = raw_data.copy(deep=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "Label Encoding assigns a **unique** integer to each category. The categories are converted into numbers, starting from 0 and going up to N-1, where N is the number of categories. \n",
    "\n",
    "Label encoding is a **straightforward** approach and easy to implement. It also doesn’t **expand** the feature space like One-Hot Encoding, which is beneficial for models that struggle with **high-dimensional** data.\n",
    "\n",
    "But it has some drawbacks such as: \n",
    "- It can introduce a new problem of ordinality when there isn't one. For example, the model might interpret the category encoded as 2 to be twice as significant as the category encoded as 1.\n",
    "\n",
    "- It's not suitable for linear models unless the categorical variable is **ordinal**.\n",
    "\n",
    "\n",
    "Remember, it's crucial to use Label Encoding appropriately and be aware of its potential impact on your model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True) # drop all the null-values (just for convenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding using .map() methode in pandas \n",
    "d = {'Europa':1, 'Earth':2, 'Mars':3}\n",
    "df['HomePlanet_labelencoding'] = df.HomePlanet.map(d)\n",
    "df[['HomePlanet', 'HomePlanet_labelencoding']].head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding using LabelEncoder class\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder() \n",
    "df['HomePlanet_labelencoding'] = encoder.fit_transform(pd.DataFrame(df.HomePlanet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['HomePlanet', 'HomePlanet_labelencoding']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "**Note: This transformer should be used to encode target values, i.e. y, and not the input X. you can use OrdinalEncoder class instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrdinalEncoder  \n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "encoder = OrdinalEncoder()\n",
    "df['HomePlanet_labelencoding'] = encoder.fit_transform(pd.DataFrame(df.HomePlanet))\n",
    "df[['HomePlanet', 'HomePlanet_labelencoding']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Ordinal Encoding\n",
    "Ordinal encoding builds upon label encoding by incorporating the **inherent order** or ranking present in ordinal categorical features. Instead of assigning arbitrary numerical values to categories, ordinal encoding preserves the relative order, ensuring that the assigned values reflect the true relationship between the categories.\n",
    "\n",
    "Ordinal encoding is ideal for features like \"education level\" (e.g., high school, bachelor's, master's, doctorate), \"customer satisfaction rating\" (e.g., dissatisfied, neutral, satisfied, very satisfied), or \"movie rating\" (e.g., G, PG, PG-13, R).\n",
    "\n",
    "It ensures that the numerical values assigned to categories reflect their **relative position**. This is crucial for tasks like predicting the next category in a sequence or analyzing trends based on the order of the categories. By capturing the order information, ordinal encoding can lead to **more accurate** predictions and better model performance compared to simple label encoding.\n",
    "\n",
    "But it has its own limitations such as: \n",
    "\n",
    "* **Assumptions about equal intervals:** Ordinal encoding assumes that the intervals between adjacent categories are equal. This might not always be the case, and it's important to consider the underlying data distribution when using this technique.\n",
    "* **Sensitivity to outliers:** Outliers can disproportionately influence the assigned numerical values, potentially affecting model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data for Ordinal encoding \n",
    "data = pd.DataFrame({\"Education Level\": [\"High School\", \"Bachelor's Degree\", \"Master's Degree\", \"PhD\", \"Associate's Degree\"]})\n",
    "\n",
    "# One way is to specifies the order of the categories in dict() and use the .map() method.\n",
    "education_mapping = {\n",
    "    'High School': 0,\n",
    "    \"Associate's Degree\": 1,\n",
    "    \"Bachelor's Degree\": 2,\n",
    "    \"Master's Degree\": 3,\n",
    "    'PhD': 4\n",
    "}\n",
    "data['Education_encoding'] = data['Education Level'].map(education_mapping)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "---\n",
    "Scikit-learn provides the OrdinalEncoder class for convenient ordinal encoding. Let's explore its key parameters:\n",
    "\n",
    "* **categories** (list of arrays or sparse matrices, default=None): This parameter specifies the **order of the categories** for each feature. If not provided, the order will be inferred from the data.\n",
    "\n",
    "\n",
    "* **handle_unknown** (str, default='error'): This parameter controls how the encoder handles **unknown** categories during prediction. Options include:\n",
    "    * `'error'`: Raise an error if an unknown category is encountered.\n",
    "    * `'use_encoded_value'`: Assign a specific value (specified by `unknown_value`) to unknown categories.\n",
    "    \n",
    "    \n",
    "* **unknown_value** (int, default=None): This parameter specifies the value to be assigned to unknown categories when `handle_unknown` is set to `'use_encoded_value'`.\n",
    "\n",
    "\n",
    "* **encoded_missing_value** (int, default=None): This parameter specifies the value to be assigned to missing values (NaN) in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrdinalEncoder  \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categories = [[\"High School\", \"Associate's Degree\", \"Bachelor's Degree\", \"Master's Degree\", \"PhD\"]]\n",
    "encoder = OrdinalEncoder(categories=categories, handle_unknown='use_encoded_value', unknown_value=-1, encoded_missing_value=-1)\n",
    "\n",
    "data['Education_ordinal_encoding'] = encoder.fit_transform(pd.DataFrame(data['Education Level']))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.transform([['kddfkhg']]) # Handling unkown inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## One-Hot Encoding \n",
    "One-hot encoding transforms categorical variables into a **binary matrix** representation. It creates a binary column for each category in the original data. \n",
    "\n",
    "For example, for the columns 'HomePlanet' we have three categories (Earth, Mars, Europa) one-hot encoding will create three new features, one for each home planet. If the original feature value is ‘Earth’, then the ‘Earth’ column will have a 1, and the other two columns will have 0s.\n",
    "\n",
    "---\n",
    "**Advantages and Limitations of One-Hot encoding:**\n",
    "\n",
    "\n",
    "A clear advantage of **one-hot encoding** is its ability to reveal whether specific **unique values** within a group have a disproportionately large or significant influence, be it beneficial or detrimental. This encoding method preserves the information pertaining to each variable's values, which is not always the case with other encoding methods. In contrast, **label encoding** provides a general understanding of how a **feature** affects models overall, yet it lacks the capacity to pinpoint the distinct effects of each unique value within that feature.\n",
    "\n",
    "\n",
    "\n",
    "Understanding the **influence of each unique value** in categorical data is beneficial, but it can sometimes lead to less precise models. Particularly, if certain values appear more frequently than others, there's a risk of **overestimating their importance**. Consider a scenario where a person named Joseph, who is new to a building, coincidentally enters just as a power outage occurs. It would be unreasonable to blame him, even though the data shows a correlation. Therefore, I prefer **one-hot encoding** when the number of unique values is manageable and evenly distributed. The dataset should also be sufficiently large to handle the diversity of values without issues. A potential downside is that once we encode a feature, we might overlook the **feature's overall effect** because we focus on the individual values instead of the feature as a whole.\n",
    "\n",
    "\n",
    "Also when handling categorical data with **high cardinality**, which means the feature has a large number of unique values, **one-hot encoding** can become problematic. This is because it creates a separate column for each unique value, leading to two main issues. The first issue is the **consumption of space**, and the second, more significant issue, is the **curse of dimensionality**. To fully understand the curse of dimensionality, I would suggest to checkout this [link](#https://www.quora.com/What-is-the-curse-of-dimensionality) and learn from the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding  \n",
    "# Refresh our data\n",
    "df = raw_data.copy(deep=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot encoding using .get_dummies method from pandas \n",
    "pd.get_dummies(df, columns=['HomePlanet']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "The `get_dummies` method in pandas provides a convenient way to perform one-hot encoding directly on DataFrames. It takes a categorical column or a list of columns as input and returns a DataFrame with the one-hot encoded representation.\n",
    "\n",
    "\n",
    "Let's explore the key parameters of `get_dummies`:\n",
    "\n",
    "* **prefix:** This parameter specifies the prefix for the generated feature names. If a list is provided, it should have the same length as the number of columns being encoded.\n",
    "\n",
    "\n",
    "* **prefix_sep:** This parameter defines the separator between the prefix and the original feature name in the generated feature names.\n",
    "\n",
    "\n",
    "* **dummy_na:** This parameter controls whether to include a separate feature for missing values (NaN). If set to `True`, a feature named \"missing\" will be created for each encoded column.\n",
    "\n",
    "\n",
    "* **drop_first:** This parameter determines whether to drop the first dummy feature for each encoded column. This can be useful to avoid **multicollinearity** issues in linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# For sklearn >=1.2 use sparse_output=False\n",
    "encoder = OneHotEncoder(sparse_output=False)  # return dense array\n",
    "encoded_data = encoder.fit_transform(df[['HomePlanet']])\n",
    "\n",
    "# Create DataFrame with proper column names\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['HomePlanet']))\n",
    "\n",
    "# Concatenate with original dataframe\n",
    "df_encoded = pd.concat([df.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "---\n",
    "While both `get_dummies` and `OneHotEncoder` offer one-hot encoding capabilities, `OneHotEncoder` provides a more comprehensive and flexible solution for handling categorical features within your machine learning workflow. Its integration with scikit-learn, fine-grained control, and support for sparse matrices make it a powerful tool for preprocessing your data effectively.\n",
    "Let's delve into the key parameters of `OneHotEncoder`:\n",
    "\n",
    "* **categories:** This parameter allows you to specify the order of the categories for each feature. If not provided, the order will be inferred from the data.\n",
    "\n",
    "\n",
    "* **sparse (default=True):** This parameter determines whether the encoded data should be returned in a sparse matrix format. Sparse matrices can be more **memory-efficient** for datasets with many categories.\n",
    "\n",
    "\n",
    "* **handle_unknown (str, default='error'):** This parameter controls how the encoder handles unknown categories during prediction. Options include:\n",
    "    * `'error'`: Raise an error if an unknown category is encountered.\n",
    "    * `'ignore'`: Ignore unknown categories and proceed with encoding the known categories.\n",
    "    * `'use_encoded_value'`: Assign a specific value (specified by `unknown_value`) to unknown categories.\n",
    "    \n",
    "    \n",
    "* **unknown_value (int, default=None):** This parameter specifies the value to be assigned to unknown categories when `handle_unknown` is set to `'use_encoded_value'`.\n",
    "\n",
    "\n",
    "* **dtype (str, default='int'):** This parameter specifies the data type of the encoded features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "---\n",
    "## Count & Frequency Encoding\n",
    "Count and frequency encoding are techniques used to represent categorical features by their frequency or proportion within the dataset. They provide an alternative to one-hot encoding, especially when the number of categories is **large** or the order of categories is not significant.\n",
    "\n",
    "**Count Encoding:**\n",
    "\n",
    "Count encoding assigns each category a value equal to the number of times it appears in the dataset. This approach is straightforward and computationally efficient, making it suitable for large datasets. However, it treats all categories equally, regardless of their overall proportion.\n",
    "\n",
    "**Frequency Encoding:**\n",
    "\n",
    "Frequency encoding assigns each category a value equal to its proportion within the dataset. This approach considers the relative importance of each category based on its frequency. However, it can be sensitive to **outliers** and might not be suitable for datasets with **highly imbalanced** categories.\n",
    "\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* **Simplicity:** Both count and frequency encoding are easy to implement and understand.\n",
    "\n",
    "* **Computational efficiency:** They are computationally efficient, especially for large datasets.\n",
    "\n",
    "* **Handling large number of categories:** They can effectively handle categorical features with a large number of categories.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "* **Loss of information:** They do not capture the relationships between categories.\n",
    "\n",
    "* **Sensitivity to outliers:** Frequency encoding can be sensitive to outliers, which might affect the assigned values.\n",
    "\n",
    "* **Limited applicability:** They are not suitable for ordinal categorical features with a natural order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Encoding \n",
    "count_map = df.groupby('HomePlanet').size().to_dict()\n",
    "df['HomePlanet_count'] = df['HomePlanet'].map(count_map)\n",
    "\n",
    "# Frequency Encoding\n",
    "total_count = len(df)\n",
    "df['HomePlanet_freq'] = df['HomePlanet'].map(df['HomePlanet'].value_counts(normalize=True).to_dict())\n",
    "\n",
    "df[['HomePlanet', 'HomePlanet_count', 'HomePlanet_freq']].head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## Target Encoding\n",
    "Target encoding is a powerful technique that incorporates information about the **target variable** into the encoding of categorical features. It assigns values to categories based on their average target values, capturing the relationship between the categories and the target variable. This method can be particularly effective for **high-cardinality** features where one-hot encoding would lead to a large increase in dataset dimensionality.\n",
    "\n",
    "**Methods for Target Encoding:**\n",
    "\n",
    "* **Simple Mean Target Encoding:** This method assigns the average target value for each category as the encoded value. It's simple to implement but can be sensitive to **outliers** and **imbalanced data**.\n",
    "\n",
    "\n",
    "* **Weighted Mean Target Encoding:** In the context of target encoding, this method can be particularly useful as it combines the **category-specific mean** (Option Mean) with the **overall mean** of the target variable. This approach helps to mitigate the risk of **overfitting** that might occur if a category has a very small number of observations. By incorporating the Overall Mean, the weighted mean becomes more stable and less sensitive to outliers in the data. The formula for the weighted mean in target encoding is expressed in LaTeX as follows:\n",
    "\n",
    "$$\n",
    "\\text{Weighted Mean} = \\frac{n \\times \\text{Option Mean} + m \\times \\text{Overall Mean}}{n+m}\n",
    "$$\n",
    "\n",
    "Here, \\( n \\) represents the weight for the Option Mean, which is usually the number of observations for the specific category, and \\( m \\) is the weight for the Overall Mean, which is a user-defined parameter.\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* **Improved predictive performance:** Target encoding can significantly improve the performance of machine learning models by capturing the relationship between categories and the target variable.\n",
    "\n",
    "\n",
    "* **Handling categorical features:** It effectively handles categorical features, even with a large number of categories.\n",
    "\n",
    "\n",
    "* **Interpretability:** The encoded values provide insights into the relationship between categories and the target variable.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "* **Data leakage:** Target encoding can introduce data leakage if the target variable is used in the training data for encoding.\n",
    "\n",
    "\n",
    "* **Overfitting:** It can lead to overfitting, especially on small datasets.\n",
    "\n",
    "\n",
    "* **Computational complexity:** It can be computationally expensive for large datasets.\n",
    "\n",
    "**Mitigating Limitations:**\n",
    "\n",
    "* **Splitting data into folds:** To avoid data leakage, split the data into folds and perform target encoding within each fold using the target values from the remaining folds.\n",
    "\n",
    "\n",
    "* **Test-train splitting before encoding:** Encode the categorical features using the training data only and apply the encoding to the test data.\n",
    "\n",
    "\n",
    "* **Smoothing techniques:** Apply smoothing techniques like adding a small constant to the target values before calculating the averages.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding \n",
    "from category_encoders import TargetEncoder\n",
    "encoder = TargetEncoder()\n",
    "encoder.fit(df['HomePlanet'], df.Transported) \n",
    "df['HomePlanet_Encoded'] = encoder.transform(df['HomePlanet'])\n",
    "df[['HomePlanet', 'HomePlanet_Encoded', 'Transported']].head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "Here's an explanation some of `TargetEncoder`'s parameters:\n",
    "\n",
    "- **verbose**: An integer that controls the verbosity of the output. A value of 0 means no output.\n",
    "\n",
    "\n",
    "- **cols**: A list of columns to encode. If set to None, all string columns will be encoded.\n",
    "\n",
    "\n",
    "- **drop_invariant**: A boolean that determines whether to drop columns with 0 variance.\n",
    "\n",
    "\n",
    "- **return_df**: A boolean that specifies whether to return a pandas DataFrame from the transform method. If set to False, a numpy array will be returned.\n",
    "\n",
    "\n",
    "- **handle_missing**: A string that specifies how to handle missing values. Options are 'error', 'return_nan', and 'value'. The default 'value' returns the target mean.\n",
    "\n",
    "\n",
    "- **handle_unknown**: A string that specifies how to handle unknown categories during the transform. Options are 'error', 'return_nan', and 'value'. The default 'value' returns the target mean.\n",
    "\n",
    "\n",
    "- **smoothing**: A float that adds a smoothing effect to balance the categorical average vs the prior. It must be strictly greater than 0. Higher values result in a flatter S-curve.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[Table Of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "# Feature Scaling \n",
    "\n",
    "\n",
    "Feature scaling is a data preprocessing technique used to transform the values of features or variables in a dataset to a similar scale. The purpose is to ensure that all features **contribute equally** to the model and to avoid the **domination** of features with **larger values**.\n",
    "It becomes necessary when dealing with datasets containing features that have different **ranges**, **units of measurement**, or **orders of magnitude**.\n",
    "\n",
    "**Why Should We Use Feature Scaling?**\n",
    "\n",
    "\n",
    "   - **Improved Model Accuracy**: Some machine learning algorithms are **sensitive** to feature scaling, while others are virtually invariant. Algorithms like linear regression, logistic regression, neural networks, and PCA (principal component analysis) require data to be scaled. For example, gradient descent-based algorithms benefit from feature scaling to ensure smooth convergence.\n",
    "   \n",
    "   \n",
    "   - **Enhanced Interpretability**: Transforming features onto a common scale makes data more interpretable. Without scaling, comparing two features becomes challenging due to scale differences.\n",
    "   \n",
    "   \n",
    "   - **Faster Convergence**: Optimization algorithms (e.g., gradient descent) converge faster when features are scaled. Consistent step sizes across features prevent one feature from dominating the learning process.\n",
    "   \n",
    "   \n",
    "   - **Reduced Computational Resources**: Properly scaled data reduces the computational resources required for training the model.\n",
    "   \n",
    "   \n",
    "   - **Visual Appeal**: Scaled data is visually appealing, especially when creating visualizations or plots.\n",
    "   \n",
    "\n",
    "### Common Techniques for Feature Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "# Numerical columns \n",
    "num_columns = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data.copy(deep=True)\n",
    "df.describe() # Original data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "### Standardization (Z-score normalization):\n",
    "This method transforms features to have a mean of 0 and a standard deviation of 1. It's suitable for algorithms that rely on Euclidean distance and is often used in conjunction with regularization techniques. \n",
    "  \n",
    "  $$ z = \\frac{{x - \\mu}}{{\\sigma}} $$\n",
    "where:\n",
    "- $z$ is the standardized value.\n",
    "- $x$ is the original feature value.\n",
    "- $\\mu$ is the mean of the feature.\n",
    "- $\\sigma$ is the standard deviation of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization (Z-score normalization)\n",
    "standard_scaler = StandardScaler()\n",
    "pd.DataFrame(standard_scaler.fit_transform(df[num_columns]), columns=num_columns).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "**Note:** When you scale data using `sklearn`'s `StandardScaler`, the mean of the scaled data should theoretically be zero. However, in practice, due to the limitations of **floating-point precision**, the mean may not be exactly zero but very close to it. \n",
    "\n",
    "The values you're seeing, such as `-1.419082e-17`, are extremely close to zero. They are essentially zero within the precision limits of floating-point arithmetic. This is a common result of the finite precision with which computers store and manipulate decimal numbers, known as **machine epsilon**. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### Normalization (Min-Max scaling):\n",
    "This method scales features to a range between 0 and 1. It's a simple and effective method for many algorithms but can be sensitive to outliers. \n",
    "  \n",
    "  $$ x_{\\text{normalized}} = \\frac{{x - x_{\\text{min}}}}{{x_{\\text{max}} - x_{\\text{min}}}} $$\n",
    "  \n",
    "where:\n",
    "- $x_{\\text{normalized}}$ is the normalized value.\n",
    "- $x_{\\text{min}}$ is the minimum value of the feature.\n",
    "- $x_{\\text{max}}$is the maximum value of the feature.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization (Min-Max scaling)\n",
    "minmax_scaler = MinMaxScaler()\n",
    "pd.DataFrame(minmax_scaler.fit_transform(df[num_columns]), columns=num_columns).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "You can change the range of your data from (0,1) to any range with `feature_range` parameter\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### RobustScaler:\n",
    "\n",
    "This method scales features using the median and interquartile range (IQR) to make it less sensitive to outliers compared to standardization. \n",
    "\n",
    "\n",
    "$$x_{\\text{robust}} = \\frac{{x - \\text{median}(x)}}{{\\text{IQR}(x)}}$$\n",
    "  \n",
    "       \n",
    "where:\n",
    "- $x_{\\text{robust}}$ is the robustly scaled value.\n",
    "- $\\text{median}(x)$ is the median of the feature.\n",
    "- $\\text{IQR}(x)$ is the interquartile range of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "pd.DataFrame(robust_scaler.fit_transform(df[num_columns]), columns=num_columns).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### MaxAbsScaler:\n",
    "This method scales features by dividing them by the maximum absolute value in the dataset. It's useful for sparse datasets and algorithms that are sensitive to feature magnitudes.\n",
    "  \n",
    "$$ x_{\\text{maxabs}} = \\frac{x}{{\\max(|x|)}} $$\n",
    "\n",
    "\n",
    "where:\n",
    "- $x_{\\text{maxabs}}$ is the scaled value using MaxAbsScaler.\n",
    "- $\\max(|x|)$ is the maximum absolute value of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxAbsScaler\n",
    "maxabs_scaler = MaxAbsScaler()\n",
    "pd.DataFrame(maxabs_scaler.fit_transform(df[num_columns]), columns=num_columns).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "[Table Of Contents](#Table-of-Contents)\n",
    "\n",
    "# Imbalanced Datasets\n",
    "\n",
    "Imagine training a model to identify rare diseases. The data you have might contain thousands of healthy patient records but only a handful of records for the rare disease. This disparity in class representation is what makes a dataset imbalanced. \n",
    "\n",
    "Imbalanced datasets pose significant challenges for machine learning models: \n",
    "\n",
    "- **Biased Predictions**: Models trained on imbalanced data tend to favor the majority class, neglecting the minority class. This leads to poor performance on the minority class, often resulting in false negatives and missed opportunities for critical events like fraud detection or medical diagnosis.\n",
    "\n",
    "\n",
    "- **Poor Generalization**: Imbalanced datasets can lead to poor generalization because the model doesn’t learn enough about the minority class.\n",
    "\n",
    "\n",
    "- **Misleading Evaluation Metrics**: Traditional metrics like accuracy can be misleading for imbalanced datasets. A model might achieve high accuracy by simply predicting the majority class for every instance, even if it completely ignores the minority class.\n",
    "\n",
    "for this one we need to use an imbalanced dataset like [`Adult Dataset`](#https://archive.ics.uci.edu/dataset/2/adult). This is a classic machine learning dataset with 48,842 instances and 14 features. Its primary task is to predict whether an individual’s income exceeds $50,000 per year based on census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Load the data \n",
    "adult_data = pd.read_csv('./adult.csv')\n",
    "df = adult_data.copy()\n",
    "\n",
    "# Some extra preprocessing \n",
    "df.columns = [i.strip() for i in df.columns]\n",
    "df.replace(' ?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your data for imbalanced data probelm \n",
    "df['income'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "let's build a trivial model that returns only the label of the majority class as the predicted label regardless of the input. Then we’ll assess the model’s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def simple_model(input_data): \n",
    "    return ' <=50K' # predicts the majority class label \n",
    "\n",
    "predictions = df.apply(lambda x: simple_model(x), axis=1)\n",
    "accuracy = accuracy_score(df['income'], predictions)\n",
    "\n",
    "print(f'Accuracy of the simple model: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "You can see  we can attain an accuracy score of **0.76** without much effort.. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "---\n",
    "Now that we understand the challenges, let's explore various solutions to address imbalanced data:\n",
    "\n",
    "## Sampling Methods \n",
    "\n",
    "### Random Oversampling\n",
    "Random oversampling involves creating **additional synthetic** instances of the minority class by **randomly duplicating** existing examples. Essentially, we \"inflate\" the minority class to balance the dataset.\n",
    "\n",
    "**Advantages**:\n",
    "  - **Increased Representation**: By adding more minority class samples, we ensure that the model learns from a more balanced dataset.\n",
    "    \n",
    "    \n",
    "  - **Preserves Information**: No loss of information since we're using existing data points.\n",
    "    \n",
    "**Disadvantages**:\n",
    "  - **Risk of Overfitting**: Replicating minority instances can lead to overfitting, especially if the dataset is already small.\n",
    "    \n",
    "    \n",
    "  - **Data Leakage**: If the same instances appear in both the training and validation/test sets, it can lead to data leakage.\n",
    "\n",
    "\n",
    "### Random Undersampling\n",
    "Random undersampling involves randomly removing instances from the majority class to balance the dataset. We reduce the number of majority class samples.\n",
    "\n",
    "**Advantages**:\n",
    "- **Faster Training**: Smaller dataset means faster training times.\n",
    "    \n",
    "    \n",
    "- **Reduced Risk of Overfitting**: Fewer majority class instances mitigate the risk of overfitting.\n",
    "    \n",
    "**Disadvantages**:\n",
    "- **Loss of Information**: Removing instances may discard valuable information.\n",
    "    \n",
    "    \n",
    "- **Bias**: Undersampling can lead to a biased model if important majority class examples are removed.\n",
    "\n",
    "Remember that the choice between oversampling and undersampling depends on the **specific problem**, **dataset size**, and **available resources**. Often, a combination of both techniques (**hybrid sampling**) yields better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an exercise, write the code of the above methods using .sample() from pandas or resample class from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "Synthetic Minority Oversampling Technique (SMOTE), is a popular technique for addressing imbalanced datasets. It tackles the issue by artificially generating synthetic data points for the minority class, effectively balancing the dataset and improving the model's performance.\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "1. **`Identify the Minority Class`:** Begin by identifying the class with the least number of instances in your dataset. In our example, the minority class is ' >50K' in the 'salary' column of the 'df' dataframe.\n",
    "\n",
    "2. **`Select a Data Point`:** Randomly select a data point from the minority class.\n",
    "\n",
    "3. **`Find its Nearest Neighbors`:** Identify the k nearest neighbors of the selected data point within the minority class. This involves calculating the distance between the selected point and all other points in the minority class, and then choosing the k points with the smallest distances.\n",
    "\n",
    "4. **`Generate a Synthetic Data Point`:** Create a new synthetic data point by randomly selecting a point from the k nearest neighbors and interpolating between the two points. This involves taking a weighted average of the features of the selected point and its neighbor.\n",
    "\n",
    "5. **`Repeat`:** Repeat steps 2-4 to generate additional synthetic data points until the desired level of oversampling is achieved.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* **Addresses Class Imbalance:** SMOTE effectively increases the representation of the minority class, leading to a more balanced dataset.\n",
    "\n",
    "\n",
    "* **Improves Model Performance:** By providing more data points for the minority class, SMOTE helps the model learn the characteristics of this class better, leading to improved performance on the minority class.\n",
    "\n",
    "\n",
    "* **Preserves Information:** Synthetic samples are created by interpolating between real samples, preserving the underlying data distribution.\n",
    "\n",
    "**Disadvantages:** \n",
    "\n",
    "* **Overgeneralization:** SMOTE can sometimes create noisy synthetic samples that do not accurately represent the minority class.\n",
    "\n",
    "\n",
    "* **May Not Capture All Variations:** Synthetic data points generated by SMOTE might not capture the full range of variations present in the real minority class data.\n",
    "\n",
    "\n",
    "* **Computational Cost:** Depending on the size of the dataset and the number of synthetic data points generated, SMOTE can be computationally expensive.\n",
    "\n",
    "\n",
    "- **Dependency on k**: The choice of the number of nearest neighbors (k) affects the quality of synthetic samples.\n",
    "\n",
    "\n",
    "- **Doesn't Address Data Quality Issues**: SMOTE does not fix issues related to mislabeled or noisy data.\n",
    "\n",
    "\n",
    "Remember to carefully evaluate the impact of SMOTE on your model's performance and consider other techniques in conjunction for a comprehensive approach to handling imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Notice that the algorithm expects numerical input\n",
    "# Initialize SMOTE with desired parameters\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship',\n",
    "       'race', 'gender', 'native-country']\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Now 'df' has label encoded categorical features\n",
    "df[cat_features] = df[cat_features].apply(lambda col: le.fit_transform(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "**NOTE:** This approach is simply for illustrative purposes. For detailed guidance, please refer to the [‘Encoding Categorical Features’](#Categorical-Feature-Encoding) section in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to generate synthetic samples\n",
    "\n",
    "X = df.drop('income', axis=1)  # Features\n",
    "y = df['income']               # Class labels\n",
    "\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "resampled_df = pd.concat((X_resampled, y_resampled), axis=1)\n",
    "\n",
    "# Now you can see that the data is balanced. \n",
    "resampled_df['income'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "The **SMOTE class** in the `imbalanced-learn` library is a powerful tool designed to tackle the problem of imbalanced datasets. Here's an overview of its important parameters:\n",
    "\n",
    "- `sampling_strategy`: Determines the sampling strategy to use. It can be a float, string, dictionary, or callable. When set to 'auto', it resamples all classes except the majority class to have the same number of samples.\n",
    "\n",
    "\n",
    "- `k_neighbors`: Specifies the number of nearest neighbors to use when constructing synthetic samples. It can be an integer or an object that is an instance of a nearest neighbors algorithm from `sklearn.neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "### ADASYN\n",
    "\n",
    "Adaptive Synthetic Sampling (ADASYN) is a technique used to address imbalanced datasets in machine learning, specifically improving classification performance for underrepresented classes. It is an extension of the **SMOTE** algorithm. Unlike SMOTE, which uniformly oversamples all minority instances, ADASYN focuses on generating synthetic examples for the minority class based on the **difficulty of classifying** them correctly.\n",
    "\n",
    "\n",
    "**How Does ADASYN Work?**\n",
    "\n",
    "1. **`Calculate Imbalance Ratio`**: Determine the degree of imbalance by calculating the ratio of the number of instances in the minority class to the majority class.\n",
    "\n",
    "\n",
    "2. **`Identify Neighbors`**: For each instance in the minority class, identify its k-nearest neighbors from the same class.\n",
    "\n",
    "\n",
    "3. **`Calculate Weights`**: Assign a weight to each minority class instance based on the number of majority class neighbors. The more majority class neighbors, the higher the weight, indicating a harder-to-classify instance.\n",
    "\n",
    "\n",
    "4. **`Generate Synthetic Samples`**: For each minority class instance, generate synthetic samples. The number of samples is proportional to the assigned weight, focusing on instances that are harder to classify.\n",
    "\n",
    "\n",
    "5. **`Construct Synthetic Instances`**: Create new synthetic instances by interpolating between the minority class instance and its neighbors. This is done by randomly selecting one of the k-nearest neighbors and creating a sample along the line connecting the instance and its neighbor.\n",
    "\n",
    "\n",
    "6. **`Repeat`**: Repeat the process until the dataset is sufficiently balanced.\n",
    "\n",
    "\n",
    "\n",
    "**Advantages of ADASYN:**\n",
    "\n",
    "- **Adaptive Oversampling**: ADASYN generates synthetic samples based on the **degree** of class imbalance and the **difficulty** of learning the minority class instances. This makes it more effective than simple oversampling techniques, such as random oversampling.\n",
    "\n",
    "\n",
    "- **Avoids Overfitting**: By generating diverse synthetic samples, ADASYN reduces the risk of overfitting to the minority class.\n",
    "\n",
    "\n",
    "**Disadvantages of ADASYN:**\n",
    "\n",
    "- **Computational Cost**: ADASYN requires calculating distances to nearest neighbors, which can be computationally expensive.\n",
    "\n",
    "\n",
    "- **Sensitive to Noise**: If the nearest neighbors contain noisy or mislabeled samples, ADASYN may generate poor-quality synthetic data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "\n",
    "# Apply ADASYN to the dataset\n",
    "adasyn = ADASYN(sampling_strategy='minority')\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
    "\n",
    "resampled_df = pd.concat((X_resampled, y_resampled), axis=1)\n",
    "\n",
    "# Now you can see that the data is balanced. \n",
    "resampled_df['income'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "[Table Of Contents](#Table-of-Contents)\n",
    "\n",
    "## Choosing the Proper Algorithm\n",
    "\n",
    "Another approach to dealing with imbalanced datasets is to choose an algorithm that is inherently robust to class imbalance.  For instance, tree-based algorithms like **Random Forest** and **Gradient Boosting** tend to perform better on imbalanced datasets because their hierarchical structure allows them to learn signals from both classes. \n",
    "\n",
    "This approach doesn't require any additional computational resources or preprocessing steps, making it efficient. But it does not adapt the algorithm to the specific characteristics of the imbalanced dataset, and may not be as effective as other techniques.\n",
    "\n",
    "\n",
    "## Modifying the Loss Function\n",
    "\n",
    "The loss function tells the algorithm how well it is performing, and by modifying it, we can make the algorithm pay more attention to the minority class. This can be done by introducing weights to the loss function, making errors in the minority class more **significant** than errors in the majority class. By heavily penalizing misclassifications of the minority class, the model is **incentivized** to learn **better representations** of the minority class, leading to improved performance.\n",
    "\n",
    "For example, in **logistic regression**, you can use a weighted version of the log loss that gives higher weight to the minority class. In deep learning, you can modify the **cross-entropy loss** to have class weights or even use **focal loss**, which adds a factor to down-weight easy examples and focus training on hard negatives.\n",
    "\n",
    "Sometimes, predefined loss functions may not be suitable for your specific problem. In such cases, you can create a **custom loss function** that better reflects the importance of correctly predicting the minority class. This requires a deep understanding of the problem and the data, as well as the ability to implement and test the new loss function.\n",
    "\n",
    "Modifying the loss function allows the model to be specifically **adapted** to the **characteristics** of the imbalanced dataset, potentially leading to **better performance**. Also, this approach can be applied to a **wide range** of models and algorithms, providing more **flexibility** than choosing a specific algorithm.\n",
    "\n",
    "In contrast, this approach can be **more complex** and may require extensive **domain knowledge**. Additionally, finding the **right balance** of weights can pose a challenge. **Overfitting** remains a concern as well; the model might become excessively focused on the minority class, potentially at the expense of the majority class.\n",
    "\n",
    "## Confusion Matrix \n",
    "The confusion matrix is a fundamental tool for evaluating the performance of machine learning models, it is a table that summarizes the performance of a classification model. \n",
    "\n",
    "It shows the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) for a given classification problem.\n",
    "\n",
    "The confusion matrix looks like this:\n",
    "\n",
    "|         | Predicted Positive | Predicted Negative |\n",
    "|---------|-------------------|-------------------|\n",
    "| Actual Positive | True Positive (TP) | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "\n",
    "In **imbalanced datasets**, the confusion matrix is particularly important because it provides a more comprehensive view of the model's performance than simple accuracy metrics. Accuracy alone can be misleading in imbalanced datasets, as the model may achieve a high accuracy by simply predicting the majority class all the time.\n",
    "\n",
    "The confusion matrix allows you to assess the model's performance on both the majority and minority classes, which is crucial in imbalanced scenarios. This information can help you make informed decisions about model selection, feature engineering, and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# We brought back our trivial model\n",
    "def simple_model(input_data):\n",
    "    return ' <=50K'\n",
    "\n",
    "predictions = df.apply(lambda x: simple_model(x), axis=1)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(df['income'], predictions)\n",
    "print(f'Accuracy of the simple model: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(df['income'], predictions, labels=['<=50K', '>50K'])\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[' <=50K', ' >50K'], yticklabels=[' <=50K', ' >50K'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "This figure indicates that the model is **biased** towards predicting everyone as belonging to the `<=50K` class. Despite the accuracy score of **0.76**, this doesn't mean the model is good at distinguishing between the two classes, which is the actual purpose of a classification model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "[Table Of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "# Outlier Detection \n",
    "\n",
    "\n",
    "An outlier is an observation that lies an **abnormal distance** from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations.\n",
    "\n",
    "\n",
    "\n",
    "### Why Outlier Detection is Important?\n",
    "\n",
    "Outliers can significantly **skew** the results of data analysis and statistical modeling. They can affect the **mean** and **standard deviation** of the datasets, leading to misleading representations and interpretations. \n",
    "\n",
    "In machine learning, outliers can adversely impact the training process, resulting in a model that doesn't perform well. Therefore, **detecting and handling outliers** is essential to improve the **accuracy and reliability** of data insights and predictive models.\n",
    "\n",
    "\n",
    "### Approaches to Treat Outliers\n",
    "\n",
    "- **`Treating as Missing Values`**: Outliers can be treated as **missing data** and imputed based on the rest of the dataset. For example if an outlier seems to be a result of a **data entry error**, it can be replaced with the mean or median of the data.\n",
    "\n",
    "\n",
    "- **`Trimming`**: This involves **removing** outliers from the dataset.\n",
    "\n",
    "\n",
    "- **`Capping`**:  Outliers can be capped, which involves setting a maximum and/or minimum value for the data, effectively **limiting** the influence of **extreme values**.\n",
    "\n",
    "\n",
    "- **`Binning`**: Data is divided into **bins**, and outliers can be segregated into their bins or adjusted. Effectively **reducing** the impact of extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "\n",
    "### Ways to Detect Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 0 (original data)\n",
    "df['hours-per-week'].plot(kind='box', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "### Z-Score Method: \n",
    "\n",
    "The z-score represents the number of standard deviations a data point is from the mean. A common threshold is a z-score of **3 or -3**, beyond which data points are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score \n",
    "mu = df['hours-per-week'].mean()\n",
    "std = df['hours-per-week'].std()\n",
    "upper_limit = mu + 3 * std \n",
    "lower_limit = mu - 3 * std \n",
    "# Trimming the outliers using conditional selection\n",
    "z_df_trimed = df[(df['hours-per-week'] < upper_limit) & (df['hours-per-week'] > lower_limit)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df) - len(z_df_trimed)} data points were removed using trimming\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_df_trimed['hours-per-week'].plot(kind='box', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handlign outilers using capping method \n",
    "z_df_cap = df.copy(deep=True)\n",
    "z_df_cap['hours-per-week'] = np.where(df['hours-per-week'] > upper_limit, upper_limit, np.where(df['hours-per-week'] < lower_limit, lower_limit, df['hours-per-week']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "**NOTE:** `np.where(condition, x, y)` function checks a condition for each element in an array. If the condition is `True`, it returns `x`; if `False`, it returns `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df) - len(z_df_cap)} data points were removed using Capping\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_df_cap['hours-per-week'].plot(kind='box', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "### IQR-Based Method: \n",
    "\n",
    "The Interquartile Range (IQR) is the difference between the **75th** and **25th** percentiles. An outlier is typically defined as a data point that is **1.5** times the IQR above the 75th percentile or below the 25th percentile. (The same approach that Box Plot used to identify outliers in any plot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examlpe of IQR based \n",
    "Q1 = df['hours-per-week'].quantile(0.25)\n",
    "Q3 = df['hours-per-week'].quantile(0.75)\n",
    "\n",
    "IQR = Q3 - Q1 \n",
    "upper_limit = Q3 + 1.5 * IQR \n",
    "lower_limit = Q1 - 1.5 * IQR \n",
    "\n",
    "# Handlign outilers using capping method \n",
    "iqr_df_cap = df.copy(deep=True)\n",
    "iqr_df_cap['hours-per-week'] = np.where(df['hours-per-week'] > upper_limit, upper_limit, np.where(df['hours-per-week'] < lower_limit, lower_limit, df['hours-per-week']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df) - len(iqr_df_cap)} data points were removed using Capping\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_df_cap['hours-per-week'].plot(kind='box', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "### Percentile Method: \n",
    " \n",
    "The percentile method identifies outliers as data points that fall outside a specified percentile range, such as the **1st** and **99th** percentiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Percentile Method\n",
    "upper_limit = df['hours-per-week'].quantile(0.99)\n",
    "lower_limit = df['hours-per-week'].quantile(0.01)\n",
    "\n",
    "# Trimming the outliers using conditional selection\n",
    "p_df_trimed = df[(df['hours-per-week'] < upper_limit) & (df['hours-per-week'] > lower_limit)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df) - len(p_df_trimed)} data points were removed using trimming\" )\n",
    "\n",
    "print(f\"\\n{len(z_df_trimed) - len(p_df_trimed)} more data points were removed using the percentile method than the z-score method\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df_trimed['hours-per-week'].plot(kind='box', figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A placeholder for more practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "[Table Of Contents](#Table-of-Contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
